\chapter{Detailed Approach}

\textit{This section provides research questions, as well as project goals, assumptions, project risks, deliverables, and the activities that are to be performed during the research project.}

\section{Research Questions}
This research project attempts to implement BDD operations for multicore clusters. Implementing them is not trivial, since ideally both CPU and memory capacity of all participating clusters has to be utilized as efficiently as possible. A perfect solution would split work perfectly across the participating processes such that every processing element only needs to access its own local memory. This however cannot easily be achieved, if possible at all. So the first step in this research project is to find out how to deal with this problem. This raises the following three research questions.

\begin{enumerate}
	\item[Q.1]{\textit{How can BDDs be spread over all participating clusters so that the memory of all clusters is used efficiently?}}
	\item[Q.2]{\textit{How can work be partitioned such that every participating process can work as efficiently as possible?}}
	\item[Q.3]{\textit{How can load-balancing be implemented such that the number of network operations is minimized?}}
\end{enumerate}

An answer to these three research questions is given in this report. The goal of the graduation project is to implement and benchmark the BDD operations based on the related work given in this report. Benchmarking is done to test the efficiency of the implementation. The following research questions need to be answered after the implementation and benchmarking phase phase.

\begin{enumerate}
	\item[Q.4]{\textit{How well does the implementation scale across the number of participating CPU cores?}}
	\item[Q.5]{\textit{How well does the implementation scale across the number of participating machines?}}
	\item[Q.6]{\textit{How can the scalability of the implementation be improved?}}
\end{enumerate}

If time permits the result of research question \textit{Q.6} can be used to make the implementation more efficient. This introduces another iteration of implementation and benchmarking (which requires the measurement plan to be re-applied). Also the answers given to the research questions can be adjusted, based on the results found in the second iteration. If time runs short, improving the implementation can be marked as future work.

At the end of the research project a conclusion must be given based on the experimental results, which answers the following main research question. 

\begin{enumerate}
	\item[Q.7]{\textit{Can BDD operations be implemented to perform efficiently on a cluster of multicore machines?}}
\end{enumerate}

Note that \textit{performing efficiently} means utilizing the available hardware resources efficiently. Efficiency is something that can be measured, so a relevant answer to this question can be given in the conclusion of the research project.

\section{Goals and Objectives}
The primary goal of this research project is to create an efficient implementation for BDD operations on multicore clusters that scales well across the number of processes and the number of participating clusters. 

The second goal is to create a paper out of the results obtained from the research. The relevance of these results has already been discussed in the motivational part of this report.

The third goal, which is a personal goal, is to try to obtain a PhD funding during or after the graduation project in order to continue working on heterogeneous algorithms.

\section{Methods and Techniques}
The programming language C is used to create the implementation. MVAPICH2-X 2.1a in combination with OpenSHMEM is used as a hybrid PGAS/MPI communication model. For work stealing, the Scioto framework is used. The BDD operations are simply adjusted versions taken from the multicore BDD package Sylvan.

Benchmarking the communication model can be done by simply sending and receiving one-sided RDMA operations. The distributed hash table can be microbenchmarked with the \emph{YCSB} benchmark \cite{cooper2010benchmarking}. The work stealing implementation can be tested by running a parallel version of the recursive Fibonacci algorithm. The implementation of the BDD operations can be benchmarked by using the entire \emph{BEEM} database \cite{pelanek2007beem}.

\section{Assumptions}
For this research project it is assumed that a cluster of machines, connected via Infiniband, is available for experimental use at the University of Twente. It is also assumed that MVAPICH2-X 2.1a and OpenSHMEM can be installed on the cluster, so that a hybrid PGAS/MPI model can be created. 

\section{Risks}
If MVAPICH2-X 2.1a can not be installed on the cluster, an alternative framework for PGAS/MPI has to be used. This would be a problem, since MVAPICH2-X 2.1a is, to our observation, the best framework for the hybrid PGAS/MPI model on Infiniband hardware. There are not many other frameworks that support the PGAS/MPI model. We could attempt to use UPC \cite{upc} or OpenSHMEM \cite{openshmem} without MVAPICH2-X 2.1a and use only PGAS. The MPI part can then be done with another message passing framework. This however has impact on the performance of the implementation, since MVAPICH2-X 2.1a is very fast.

If OpenSHMEM can not be installed, UPC can be used instead (and the other way around), so we have two alternatives for the PGAS part. Suppose that OpenSHMEM and UPC both can not be installed, then PGAS can not be used and the project has to switch to bare RDMA instead.

The researchers of FaRM \cite{farm} found out that performance decreased significantly when increasing the amount of RDMA-exposed memory. This is because the Infiniband NIC caches page tables. When the amount of exposed memory increases, the cache runs full and the NIC needs to fetch more page tables over the PCI bus. The researchers solved this by using a low-level package called PhyCo (currently unmaintained and declared dead) to increase page sizes to 2GB. This could also happen to our implementation, so we start the implementation phase by testing if RDMA works properly when scaling the amount of exposed memory. 

During an email conversation Enno told me that there were problems with the Infiniband hardware. Enno ran some benchmarks on the hardware using OpenMPI and OpenSHMEM (which is a PGAS implementation) and found an average inter-node latency of $20 \mu s$ for \texttt{get} and \texttt{put} operations. This is more than 10 times higher than the expected latency and the latency mentioned in other scientific work (including the benchmarks presented on the MVAPICH2 website). This is a big problem for the experimental phase, since the performance should be an order of magnitude better. If the hardware cannot be fixed, then the experimental results can not be used in the paper, because they are not good enough to be published.

\section{Prerequisites}
Before starting with the graduation project, MVAPICH2-X 2.1a in combination with OpenSHMEM or UPC must be installed on the cluster connected by Infiniband. If this can not be done for some reason, the risk plan described above has to be executed.

\section{Activities}
The following activities are to be performed during the research project.

\begin{itemize}
	\item Performing a literature study to find out how BDD operations can efficiently be implemented on a cluster of multicore machines.
	\item Implementing an RDMA-based lockless distributed hash table on PGAS.
	\item Microbenchmarking the distributed hash table.
	\item Implementing a work stealing protocol that works on a cluster of multicore machines.
	\item Microbenchmarking the work-stealing protocol.
	\item Presenting the results obtained from the experimental phase in an FMT lunch meeting presentation.
	\item Implementing the BDD operations on top of the distributed hash table.
	\item Benchmarking the implementation of the BDD operations.
	\item If time permits, implementing some of the possible improvements found during the experimental phase.
	\item Finishing the master thesis.
	\item Presenting the master thesis.
	\item Writing the paper containing implementation details and experimental results.
\end{itemize}

\section{Deliverables}
The activities described in the previous section result in the following deliverables.

\begin{enumerate}
	\item A literature study in which related work is discussed and research questions \textit{Q.1}, \textit{Q.2}, and \textit{Q.3} are answered.
	\item A small benchmark to test the latency and throughput of the one-sided RDMA operations.
	\item An RDMA-based distributed hash table that scales well across the available memory of all participating nodes.
	\item A report in which the performance of the distributed hash table is evaluated. This report is the result of microbenchmarking the hash table.
	\item An implementation of the BDD operations that makes use of the distributed hash table.
	\item An report that evaluates the performance of these BDD operations. This report is the result of the experimental phase of the research project.
	\item A conclusion of the experimental phase.
	\item A lunch meeting presentation in which the outcome of the experimental phase, as well as the project itself is presented to the other members of the research group.
	\item A master thesis covering all elements discussed above in this summation.
	\item A master thesis presentation.
	\item A paper in which the implementation and the obtained results are given.
\end{enumerate}