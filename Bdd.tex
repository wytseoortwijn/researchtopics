\chapter{Background Information on Binary Decision Diagrams}

\textit{This chapter provides background information on Binary Decision Diagrams.}

\section{Preliminaries}
A Binary Decision Diagram is a data structure used to efficiently represent boolean functions. BDDs were introduced by \cite{akers1978binary} in 1978 to compress large digital (boolean) functions. BDDs were further developed by Bryant \cite{bryant1986graph, bryant1992symbolic} in 1986 and 1992. The following definition is taken from \cite{sylvan_multicore_bdd}:

\begin{definition}
	An (ordered) Binary Decision Diagram (BDD) is a directed acyclic graph with the following properties:
	\begin{enumerate}
		\item There is a single root node and two terminal nodes $0$ and $1$.
		\item Each non-terminal node $p$ has a variable $var(p) = x_i$ and two outgoing edges, labeled $0$ and $1$; we write $lvl(p) = i$ and $p[v] = q$, where $v \in \{ 0, 1 \}$
		\item For each edge from node $p$ to non-terminal node $q$, $lvl(p) < lvl(q)$.
		\item There are no duplicate nodes, i.e. $\forall p \forall q .(lvl(p) = lvl(q) \wedge p[0] = q[0] \wedge p[1] = q[1]) \implies p = q$
	\end{enumerate}
\end{definition}

In Figure \ref{fig:bdd_examples} four different simple boolean functions are represented as a BDD. The rectangle-shaped nodes are the terminal nodes $0$ and $1$ and all other nodes represent a variable. Every non-terminal node $p$ has two edges, namely a thick one representing $p[1]$ and a dashed one representing $p[0]$. All paths in the BDD that lead to the terminal node $1$ makes the boolean function represented by the BDD to evaluate to true.

\begin{figure}
	\centering
	\subfloat[$x$] {
		\input{Tikz/bdd_x}
		$\vspace{360pt}$
	}
	$\hspace{36pt}$
	\subfloat[$\neg x$] {
		\input{Tikz/bdd_not_x}
	}
	$\hspace{36pt}$
	\subfloat[$x_0 \wedge x_1$] {
		\input{Tikz/bdd_x0_and_x1}
	}
	$\hspace{36pt}$
	\subfloat[$x_0 \vee x_1$] {
		\input{Tikz/bdd_x0_or_x1}
	}
	
	\caption{The BDD representation of four different simple boolean functions are displayed.}
	\label{fig:bdd_examples}
\end{figure}

It is possible to represent a set of states $S \subseteq U$ from some universe $U$ by a boolean function $f : S \rightarrow \mathbb{B}$, such that $S = \{ x \in U \ | \ f(x) \}$. If $U = \mathbb{B}^n$, then $f$ is a function with signature $f : \mathbb{B}^n \rightarrow \mathbb{B}$ and so $f$ can be represented as a BDD. 

By applying this trick the state space of some program can efficiently be represented as a BDD. Suppose that the program uses a set of variable names $X = \{ x_1, \dots, x_n \}$ and we have an assignment function $v : X \rightarrow \mathbb{B}$. Then $U = \mathbb{B}^n$ can represent all possible variable assignments under $v$ and $S = \mathbb{B}^n$ represents all reachable states. Now a boolean function $f : S = \mathbb{B}^n \rightarrow \mathbb{B}$ can be given. Suppose that the program uses a transition relation $R \subseteq S \times S$. Then the same trick can be applied again to represent $R$.

We now introduce reduced ordened BDDs.

\begin{definition}
	A \emph{Reduced Ordened BDD (ROBDD)} is an ordened BDD without redundant nodes. A node $p$ is redundent if $p[0] = p[1]$.
\end{definition}

\begin{figure}
	\centering
	\subfloat[BDD with duplicate nodes] {
		\input{Tikz/bdd_reduce_1}
		$\vspace{360pt}$
	}
	$\hspace{36pt}$
	\subfloat[BDD with a redundant node] {
		\input{Tikz/bdd_reduce_2}
	}
	$\hspace{36pt}$
	\subfloat[Reduced Ordened BDD] {
		\input{Tikz/bdd_reduce_3}
	}
	
	\caption{Three BDD representations of the boolean function $(x_0 \wedge x_1) \vee (\overline{x_0} \wedge x_1)$ (left representation), which is equivalent to $x_1$ (right representation). The rightmost BDD is obtained by removing duplicates and redundant nodes.}
	\label{fig:bdd_reductions}
\end{figure}

Figure \ref{fig:bdd_reductions} shows examples of BDDs that have duplicate nodes an redundant nodes. In Figure \ref{fig:bdd_reductions} all three BDDs are equivalent to each other, only the rightmost one is reduced and ordened. 

\section{Sequential BDD operations}
BDD operations use two tables, namely a \emph{Unique Table} and a \emph{Computed Table}. The unique table is used to store BDD nodes and is necessary to ensure that there are no duplicate nodes. The unique table is usually implemented as a hash table. The computed table acts as a cache. Most of the BDD operations are recursively defined, and the subresults are stored in the computed table. Before performing recursive calls the operations can check if the operation has been done before.

Most BDD operations are based on a concept called Shannon decomposition \cite{dijk2012parallelization}, which is defined below.

\begin{definition}[Restriction]
	Let $X = \{ x_1, \dots, x_n \}$ be a set of variable names and $\phi(x_1, \dots, x_n)$ a boolean formula (i.e. with codomain $\mathbb{B}$). Then the \emph{restriction} of $v$ to $x_i \in X$ in $\phi$, denoted by $\phi_{x_i=v}$, is defined by 
	\begin{equation}
		\phi_{x_i=v} \equiv \phi(x_1, \dots, x_{i - 1}, v, x_{i + 1}, \dots, x_n)
	\end{equation}
\end{definition}

\begin{definition}[Shannon Decomposition]
	Let $X = \{ x_1, \dots, x_n \}$ be a set of variable names and $\phi(x_1, \dots, x_n)$ a boolean formula (i.e. with codomain $\mathbb{B}$). Then the \emph{Shannon decomposition} of $\phi$ along $x \in X$ is defined as
	\begin{equation}
		\phi \equiv (x \wedge \phi_{x=1}) \vee (\overline{x} \wedge \phi_{x=0})
	\end{equation}
\end{definition}

Many BDD operations are implemented by taking some variable $x$ and recursively calculating the results of the subproblems $\phi_{x=0}$ and $\phi_{x=1}$ by making a BDD of the Shannon decomposition of $\phi$ along $x$. 

Furthermore existential quantification and substitution is needed to implement the \emph{relational product} operation.

\begin{definition}[Existential Quantification]
	Let $X = \{ x_1, \dots, x_n \}$ be a set of variable names and $\phi$ be a boolean function. Take $x \in X$. Then existential quantification is defined as $\exists x \phi = \phi_{x=0} \vee \phi_{x=1}$. Take $X' = \{ x'_1, \dots, x'_m \} \subseteq X$. Then $\exists X' \phi \equiv \exists x'_1, \dots, \exists x'_m \phi$.
\end{definition}

\begin{definition}[Substitution]
	Let $X = \{ x_1, \dots, x_n \}$ be a set of variable names and $\phi$ be a boolean function. Take $x_i, y \in X$. Then substitution is defined by:
	\begin{equation}
		\phi[x_i \gets y] \equiv \phi(x_1, \dots, x_{i - 1}, y, x_{i + 1}, \dots, x_n)
	\end{equation}
	Now consider two sets $Y = \{ y_1, \dots, y_m \} \subseteq X$ and $Z = \{ z_1, \dots, z_m \} \subseteq X$ of variable names. Then $\phi[Y \gets Z]$ is defined as:
	\begin{equation}
		\phi[Y \gets Z] \equiv (((\phi[y_1 \gets z_1])[y_2 \gets z_2]) \dots [y_m \gets z_m])
	\end{equation}
\end{definition}

The following two sections describe the two basic BDD operations \texttt{ite} and \texttt{relprod}.

\subsection{The \texttt{ite} operation}
Many BDD operations can be expressed with the \texttt{ite} operation (which stands for if-then-else). This operation is specified as \cite{dijk2012parallelization}:
\begin{equation}
	\texttt{ite}(A, B, C) \equiv (A \wedge B) \vee (\overline A \wedge C)
\end{equation}
where $A, B, C$ are boolean formulas. Table \ref{tab:ite} shows a number of boolean formulas expressed with the \texttt{ite} operation.

\begin{table}[ht]
	\centering
	\begin{tabular}{| c | l |}
		\hline
		Operator & ITE \\ 
		\hline 
		$F \wedge G$ & $\texttt{ite}(F, G, 0)$ \\
		$F \vee G$ & $\texttt{ite}(F, 1, G)$ \\
		$F \oplus G$ & $\texttt{ite}(F, \overline G, G)$ \\
		$\neg (F \wedge G)$ & $\texttt{ite}(F, \overline G, 1)$ \\
		$\neg (F \vee G)$ & $\texttt{ite}(F, 0, \overline G)$ \\
		$F \rightarrow G$ & $\texttt{ite}(F, G, 1)$ \\
		$F \leftarrow G$ & $\texttt{ite}(F, 1, \overline G)$ \\
		$F \leftrightarrow G$ & $\texttt{ite}(F, G, \overline G)$ \\
		$\overline F \wedge G$ & $\texttt{ite}(F, 0, G)$ \\
		$F \wedge \overline G$ & $\texttt{ite}(F, \overline G, 0)$ \\
		\hline
	\end{tabular}
	\caption{This table shows several boolean formulas expressed with the \texttt{ite} operation. This table is taken from \cite{dijk2012parallelization}.}
	\label{tab:ite}
\end{table}

\begin{figure}
	\centering
	\begin{algorithm}[H]
		\SetStartEndCondition{ }{}{}%
		\SetKwProg{Fn}{bdd}{\string:}{}
		\SetKwFunction{fun}{ite}
		\SetKw{KwTo}{in}\SetKwFor{For}{for}{\string:}{}%
		\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
		\SetKwFor{While}{while}{:}{fintq}%
		\SetKw{test}{in}{}%
		\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%

		\Fn{\fun{\textbf{bdd} $A$, \textbf{bdd} $B$, \textbf{bdd} $C$}} {
			\textbf{if} $A=1$ \textbf{return} $B$ \\
			\textbf{if} $A=0$ \textbf{return} $C$ \\
			\textbf{if} $\texttt{in-cache}(A, B, C)$ \textbf{return} $\texttt{cache-lookup}(A,B,C)$ \\
			$x \gets \texttt{min}(\texttt{var}(A), \texttt{var}(B), \texttt{var}(C))$ \\
			$R_{low} \gets \texttt{ite}(\texttt{low}(A, x), \texttt{low}(B, x), \texttt{low}(C, x))$ \\
			$R_{high} \gets \texttt{ite}(\texttt{high}(A, x), \texttt{high}(B, x), \texttt{high}(C, x))$ \\
			$R \gets \texttt{mk}(x, R_{low}, R_{high})$ \\
			$\texttt{add-to-cache}(A, B, C, R)$ \\
			\Return{$R$}
		}
	\end{algorithm}

	\caption{The (sequential) implementation of the \texttt{ite} operation.}
	\label{fig:ite_seq}
\end{figure}

The implementation of the \texttt{ite} function is given in Figure \ref{fig:ite_seq}. Instead of taking three boolean formulas, the implementation takes their BDD representations. The \texttt{in-cache}, \texttt{cache-lookup}, and \texttt{add-to-cache} functions tests if the BDD is in the computed table, gets the BDD from the computed table and adds the BDD to the computed table, respectively.

The operations \texttt{low} and \texttt{high} calculate the \emph{cofactor} of their first operand. These two functions are specified as follows:
\begin{equation}
	\texttt{low}(F, x) \equiv F_{x=0}
\end{equation}
\begin{equation}
	\texttt{high}(F, x) \equiv F_{x=1}
\end{equation}

The \texttt{mk} function creates a new BDD node and adds it to the unique table.

\subsection{The \texttt{relprod} operation}
Binary Decision Diagrams are used in symbolic model checking. An important model checking algorithm is the \emph{reachability} algorithm, which determines the set of all reachable program states, given an initial state and a transition relation. 

Burch et al. \cite{burch1994symbolic} used the \emph{relational product} operation to implement the reachability algorithm, where both the initial state and the transition relation function are represented by boolean functions. The relational product has the form: 
\begin{equation}
	\texttt{relprod}(A, B, X) \equiv \exists X. (A \wedge B)
	\label{eqn:relprod}
\end{equation}
where $X$ is a set of variables and $A, B$ are boolean functions.

Let $X = \{ x_1, \dots, x_n \}$ be a set of variable names. Suppose that $\phi(x_1, \dots, x_n) \equiv \phi(X)$ represent a set of states and $\psi(x_1, \dots, x_n, x'_1, \dots, x'_n) \equiv \psi(X, X')$ represent the transition relation. Then \texttt{relprod} \ref{eqn:relprod} is used to calculate the next set of states $\phi'(X)$ by:
\begin{equation}
	\phi'(X') \equiv \exists X. (\phi(X) \wedge \psi(X, X')) \equiv \texttt{relprod}(\phi(X), \psi(X, X'), X)
	\label{eqn:relprod_1}
\end{equation}
\begin{equation}
	\phi'(X) \equiv \phi'(X')[X' \gets X]
	\label{eqn:relprod_2}
\end{equation}
Note that $\phi(X)$ is used in \ref{eqn:relprod_1} so that the successors calculated by $\psi(X, X')$ are limited to states represented by $\phi(X)$. Then all variable names of $X$ are abstracted, so that only variable names in $X'$ remain. The last step, which is performed in \ref{eqn:relprod_2}, is to rename all variable names from $X'$ to $X$. This is done by applying substitution. The result is the set of states $\phi'(X)$ reachable from $\phi(X)$ by applying the transition relation.

\begin{figure}
	\centering
	\begin{algorithm}[H]
		\SetStartEndCondition{ }{}{}%
		\SetKwProg{Fn}{bdd}{\string:}{}
		\SetKwFunction{fun}{relprod}
		\SetKw{KwTo}{in}\SetKwFor{For}{for}{\string:}{}%
		\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
		\SetKwFor{While}{while}{:}{fintq}%
		\SetKw{test}{in}{}%
		\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%

		\Fn{\fun{\textbf{bdd} $A$, \textbf{bdd} $B$, $X$}} {
			\textbf{if} $A=1 \wedge B=1$ \textbf{return} $1$ \\
			\textbf{if} $A=0 \vee B=0$ \textbf{return} $0$ \\
			\textbf{if} $\texttt{in-cache}(A, B, X)$ \textbf{return} $\texttt{cache-lookup}(A, B, X)$ \\
			$x \gets \texttt{min}(\texttt{var}(A), \texttt{var}(B))$ \\
			$R_{low} \gets \texttt{relprod}(\texttt{low}(A, x), \texttt{low}(B, x), X)$ \\
			$R_{high} \gets \texttt{relprod}(\texttt{high}(A, x), \texttt{high}(B, x), X)$ \\

			\If{$x \in X$} {
				$R \gets \texttt{ite}(R_{low}, 1, R_{high})$
			}
			\Else {
				$R \gets \texttt{mk}(x, R_{low}, R_{high})$ \\
			}
			
			$\texttt{add-to-cache}(A, B, X, R)$ \\
			\Return{$R$}
		}
	\end{algorithm}

	\caption{The sequential implementation of the \texttt{relprod} operation.}
	\label{fig:relprod_seq}
\end{figure}

Figure \ref{fig:relprod_seq} shows an implementation of the \texttt{relprod} function. The implementation of \cite{dijk2012parallelization} shows that if $x \in X$ and $R_{low} = 1$ then the result $1$ can simply be returned. This algorithm however shows the essence of the relational product implementation. \cite{dijk2012parallelization} also gives a new algorithm \texttt{relprods} that is more efficient than \texttt{relprod} because it does not create BDD nodes unnecessarily.

\subsection{Sequential Reachability}
The previous section shows how the \texttt{relprod} operations can be used to calculate the next set of states $\phi'(X)$ given an initial state $\phi(X)$ and a transition relation $\psi(X, X')$. Suppose that $\phi$ and $\psi$ are represented by the BDDs $I$ and $T$, respectively. Then the implementation for the sequential reachability algorithm \texttt{reach} is given in Figure \ref{fig:reach_seq}.

\begin{figure}
	\centering
	\begin{algorithm}[H]
		\SetStartEndCondition{ }{}{}%
		\SetKwProg{Fn}{bdd}{\string:}{}
		\SetKwFunction{fun}{reach}
		\SetKw{KwTo}{in}\SetKwFor{For}{for}{\string:}{}%
		\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
		\SetKwFor{While}{while}{:}{fintq}%
		\SetKw{test}{in}{}%
		\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%

		\Fn{\fun{\textbf{bdd} $I$, \textbf{bdd} $T$, $X$, $X'$}} {
			$R \gets I$ \\
			$P \gets 0$ \\
			\While{$R \not = P$} {
				$N \gets \texttt{relprod}(R, T, X)$ \\
				$N \gets N[X' \gets X]$ \\
				$P \gets R$ \\
				$R \gets \texttt{ite}(R, 1, N)$
			}
			\Return{$R$}
		}
	\end{algorithm}

	\caption{The sequential implementation of the reachability operation.}
	\label{fig:reach_seq}
\end{figure}

In every iteration of the while loop (lines 4 to 8) the set of next states (represented by $N$) is calculated from the current set of states (represented by $R$) and the two are combined by using the \texttt{ite} equivalent of the $\vee$ operation. Suppose that $\phi_0(X) \equiv \phi(X)$ and $\phi_{i+1}(X) \equiv \phi'_i(X) \vee \phi_i(X)$. Then the while loop at line 4 terminates when $\phi_{i} = \phi_{i+1}$ after $i+1$ iterations. At that point a fixpoint is reached and the BDD representation of $\phi_{i+1}$, which is the $R$ at line 9, is returned.

\section{Garbage Collection}
After performing BDD operations it may happen that some nodes become unused due to reduction. If the memory needs to be used efficiently, then \emph{garbage collection} is essential. The process of garbage collection removes all unused nodes from the unique table. There are several algorithms for garbage collection, which are discussed below. 

Somenzi \cite{somenzi2001efficient} pointed out that unused BDD nodes are often reused. This may have a negative impact on the performance of garbage collection, since nodes are possibly recreated. Therefore \cite{somenzi2001efficient} suggested that garbage collection should only be performend when there are enough unused nodes. Then the cost of the garbage collection algorithm can be justified.

In the next two sections the garbage collection techniques \emph{reference counter} and \emph{mark-and-sweep} are discussed.

\subsection{Reference Counting}
With reference counting, the number of references to a BDD nodes are recorded and stored in the unique table, together with the BDD node itself. Increasing and decreasing the reference counter can be done by performing a \texttt{compare-and-swap} operation on the reference counting bits of the bucket in which the corresponding BDD node is stored. 

When the reference counter of a BDD node becomes zero, then it is unused. When there are enough unused BDD nodes in the unique table, the garbage collection procedure is called, which deletes all unused BDD nodes with complexity $\mathcal{O}(n)$. Garbage collection can be made more efficient by letting the \texttt{mk} procedure reuse nodes that are currently unused. Then the expensive garbage collection procedure does not have to be called.

If reference counting is used in combination with RDMA, then updating the reference counter of a BDD may result in an extra RDMA write. Minimizing the number of RDMA calls is crucial for both performance and scalability, so we should try to combine reference counting with other BDD operations or we should try to find another garbage collection method. On the other hand, performing the actual removal of BDD nodes from the unique table can be done both individually and locally by each of the participating machines, even without the need to synchronize. 

\subsubsection{Related Work}
The parallel BDD implementations described in \cite{dijk2012parallelization} and \cite{sylvan_multicore_bdd} both use reference counting. This is justifiable since mark-and-sweep requires the entire computation to stop when garbage collection is performed and reference counting does not. This might be different in a distributed setting. 

\subsection{Mark and Sweep}
Another garbage collection scheme used in BDD manipulation is \emph{mark-and-sweep} \cite{mccarthy1960recursive}. Mark-and-sweep requires two passes. In the first pass (the marking pass), every BDD node that is referenced is marked. In the second pass (the sweeping pass) every BDD node that is not marked is removed. So every BDD node receives an extra bit in the unique table for marking and no reference counter is needed.

The advantage of mark-and-sweep is that no extra work needs to be done for garbage collection when a BDD node is referenced. All work for garbage collection is performed when the \texttt{garbage-collect} procedure is called. This may speed up BDD operations, especially when RDMA is used to implement them. 

The disadvantage of mark-and-sweep is that two passes are needed instead of one. Another disadvantage is that the entire computation stops during garbage collection. The advantages of PGAS can however be used to speed up garbage collection. Both the marking and the sweeping passes can be done in parallel. First the \texttt{garbage-collect} procedure partitions the unique table across all participating processes. In the marking pass each process takes a partition that resides in local memory. Then BDD nodes can be marked by using an RDMA \texttt{compare-and-swap} computation. In the sweep pass every process iterates over its local partition and removed every BDD node that is not marked.

\subsubsection{Lazy Sweeping}
A variant on mark-and-sweep is \emph{lazy sweeping}, also called \emph{mark-and-don't-sweep}. In lazy sweeping each bucket in the hash table is marked with some bit $b$ as either black ($b=0$) or white ($b=1$). When the load-factor $\alpha$ reaches some treshold, \texttt{garbage-collect} is called, which flips the interpretation of $b$. So $b=1$ becomes black and $b=0$ becomes white. This means that every bucket in the hash table (temporarily) becomes white. A single marking pass follows that makes all referenced buckets black again. With this technique only a single marking pass is needed instead of two passes. The marking pass can be performed in parallel by doing the same trick as in mark-and-sweep.

\subsubsection{Related Work}
The BDDNOW package \cite{bddnow}, which is a parallel BDD package, uses mark-and-sweep for garbage collection. The researchers argued that counting references in a distributed environment is costly, so mark-and-sweep should be used instead.

In \cite{chen1997breadth} hybrid breath-first and depth-first algorithms are given for BDD construction. The researchers preferred mark-and-sweep over reference counting because mark-and-sweep uses less memory. Mark-and-sweep can be implemented by adding a single bit to the unique table, and reference counting requires more bits. 

\section{Parallel BDD operations}
\begin{figure}
	\centering
	\begin{algorithm}[H]
		\SetStartEndCondition{ }{}{}%
		\SetKwProg{Fn}{bdd}{\string:}{}
		\SetKwFunction{fun}{par-ite}
		\SetKw{KwTo}{in}\SetKwFor{For}{for}{\string:}{}%
		\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
		\SetKwFor{While}{do in parallel}{:}{fintq}%
		\SetKw{test}{in}{}%
		\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%

		\Fn{\fun{\textbf{bdd} $A$, \textbf{bdd} $B$, \textbf{bdd} $C$}} {
			\textbf{if} $A=1$ \textbf{return} $B$ \\
			\textbf{if} $A=0$ \textbf{return} $C$ \\
			\textbf{if} $\texttt{in-cache}(A, B, C)$ \textbf{return} $\texttt{cache-lookup}(A,B,C)$ \\
			$x \gets \texttt{min}(\texttt{var}(A), \texttt{var}(B), \texttt{var}(C))$ \\

			\While{} {
				$R_{low} \gets \texttt{par-ite}(\texttt{low}(A, x), \texttt{low}(B, x), \texttt{low}(C, x))$ \\
				$R_{high} \gets \texttt{par-ite}(\texttt{high}(A, x), \texttt{high}(B, x), \texttt{high}(C, x))$ \\
			}

			
			$R \gets \texttt{mk}(x, R_{low}, R_{high})$ \\
			$\texttt{add-to-cache}(A, B, C, R)$ \\
			\Return{$R$}
		}
	\end{algorithm}

	\caption{The parallel implementation of the \texttt{ite} operation \cite{sylvan_multicore_bdd}.}
	\label{fig:ite_par}
\end{figure}

\begin{figure}
	\centering
	\begin{algorithm}[H]
		\SetStartEndCondition{ }{}{}%
		\SetKwProg{Fn}{bdd}{\string:}{}
		\SetKwFunction{fun}{par-relprod}
		\SetKw{KwTo}{in}\SetKwFor{For}{for}{\string:}{}%
		\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
		\SetKwFor{While}{do in parallel}{:}{fintq}%
		\SetKw{test}{in}{}%
		\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%

		\Fn{\fun{\textbf{bdd} $A$, \textbf{bdd} $B$, $X$}} {
			\textbf{if} $A=1 \wedge B=1$ \textbf{return} $1$ \\
			\textbf{if} $A=0 \vee B=0$ \textbf{return} $0$ \\
			\textbf{if} $\texttt{in-cache}(A, B, X)$ \textbf{return} $\texttt{cache-lookup}(A, B, X)$ \\
			$x \gets \texttt{min}(\texttt{var}(A), \texttt{var}(B))$ \\

			\While{} {
				$R_{low} \gets \texttt{par-relprod}(\texttt{low}(A, x), \texttt{low}(B, x), X)$ \\
				$R_{high} \gets \texttt{par-relprod}(\texttt{high}(A, x), \texttt{high}(B, x), X)$ \\
			}

			\If{$x \in X$} {
				$R \gets \texttt{ite}(R_{low}, 1, R_{high})$
			}
			\Else {
				$R \gets \texttt{mk}(x, R_{low}, R_{high})$ \\
			}
			
			$\texttt{add-to-cache}(A, B, X, R)$ \\
			\Return{$R$}
		}
	\end{algorithm}

	\caption{The parallel implementation of the \texttt{relprod} operation. \cite{sylvan_multicore_bdd}.}
	\label{fig:relprod_par}
\end{figure}

A parallel version of the \texttt{ite} and \texttt{relprod} operations is given in \ref{fig:ite_par} and \ref{fig:relprod_par}, respectively. The only difference between the sequential and the parallel versions is that the recursive calls are performed in parallel. The parallelism is implemented by using a work stealing framework. The implementation of \cite{dijk2012parallelization} uses the Wool framework \cite{faxen2010efficient} and the implementation of \cite{sylvan_multicore_bdd} uses Lace \cite{dijk2013lace}. 

The unique table and the computed table are implemented using the \emph{lockless} hash table described in \cite{so73119}. It is crucial to use a lockless hash table to implement parallel BDD operations, since locking mechanisms generally do not work well with parallelism.  

\subsection{Earlier Work}
In the 90s a lot of research has been done to implement parallel and distributed BDD operations. The effectiveness of these implementations was disappointing, since none of them seriously improved the performance compared to the sequential implementations. Some of the implementations are discussed below.

\subsubsection{BDDNOW}
The parallel BDD package BDDNOW \cite{BDDNOW:parallel_bdd_package} is a relatively old parallel BDD package targetting distributed memory machines (created in 1996). At that time networked workstations were the most widely available parallel computing resource, which motivated the research for BDD implementations on distributed memory. BDDNOW uses the \emph{Parallel Virtual Machine (PVM)} \cite{geist1994pvm} to create workers on multiple machines.

The researchers found out that reference counting in a distributed environment is costly, so they suggested the use of mark-and-sweep. The biggest problem during the project was the implementation of load-balancing, since network communication costs were expensive. Every time the size of the BDD doubles, BDDNOW redistributes all BDD nodes so that each participating process receives approximately the same number of nodes.  

The results obtained by BDDNOW were dissapointing. The only scenario that resulted in speedup was when the sequential implementation ran out of memory. A speedup could be obtained because the distributed implementation had more cache and memory available, namely the caches and memories of all participating processors. The sequential implementation had to move to disk-based storage when memory ran full, which had a negative impact on performance.

\subsubsection{Exploiting Memory Hierarchy}
Sanghavi et al. \cite{545652} (published in 1996) presented a high-performance BDD package that exploits memory hierarchy. Most BDD packages at that time used a DFS approach, creating disorderly memory access patterns. The researchers found out that caching could not effectively be applied and that performance could be improved by applying BFS, since it has better data-locality.  When main-memory ran full and the algorithm had to switch to disk-based storage, the use of BFS resulted in even more speedup. By applying BFS speedups of 100 were obtained compared to DFS implementations when main-memory ran full.

The researchers considered the use of \emph{Quasi-Reduced BDDs (QRBDDs)} to improve data-locality, but the size of QRBDDs is multiple times larger than normal BDDs, which makes QRBDDs impractical to work with. Another possibility to improve data-locality was to use a block-index table, but finding the index of variables appearently causes significant overhead. The researchers found a way to implement an index-based customized memory manager that maps BDD nodes to a single cache line without causing much overhead.

The paper describes two other techniques called \emph{superscalarity} and \emph{pipelining} to amortize the costs of page faults for accessing the unique table. Pipelining improves the amount of caching by performing BDD operations even when the operands are yet to be computed and superscalarity performs multiple independent BDD-operations concurrently.

\subsubsection{Implementation of an Efficient Parallel BDD Package}
Stornetta et al. \cite{Stornetta96implementationof} (also published in 1996) present a parallel BDD package that uses a non-shared distributed memory environment, very similar to \cite{BDDNOW:parallel_bdd_package}. The package uses a BFS approach, just like \cite{545652}. A distributed \emph{two-level hash table} and an uncomputed cache are used to reduce communication costs as much as possible. A two-level hash table is a hash table in which every bucket is again a hash table. The entries of the nested hash tables are called \emph{blocks}. The advantage is that every \texttt{lookup} operation retrieves multiple blocks, possibly reducing network communication. 

The \emph{uncomputed cache} is used to store ongoing recursive BDD operations. This is needed since appearently intermediate results are not stored in the computed table during recursion, but they can of course be reused. These results are stored in a seperate table, called the \emph{uncomputed table}.

Every node occuring in the BDD is owned by one of the participating workers. In case of an operation, for example $\texttt{ite}(A,B,C)$, the \emph{highest} variable in $\{ A, B, C \}$ is determined and the worker owning that variable computes $\texttt{ite}(A,B,C)$. This process is repeated during recursion. Data locality can be exploited by distributing the computed table and uncomputed cache based on variable ownership, which further reduces communication costs.

The researchers found out by experimentation that the parallel implementation has a superlinear speedup compared to the sequential version when main-memory runs full, despite the expensive communication costs. If the BDD fits into the memory of a single system, the sequential algorithm is of course much faster.

\subsubsection{BDD Operations Using a Distributed Shared Memory}
Parasuram et al. \cite{parasuram1994parallel} (published in 1994) describes parallel BDD operations for distributed shared memory environments. The BDD operations exploit fine-grained parallelism. Inter-node communication is optimized by distributing the smallest units of data and computational load to the participating processes. Since exploiting data-locality and predicting which nodes are involved in a computation is difficult, the researchers argued that this is a good solution. 

Load-balancing is implemented by using \emph{distributed stacks}. Each worker owns a stack and is able to perform $\texttt{push}$ and $\texttt{pop}$ operations on the stacks of other workers. If one of the workers runs out of work, the other workers push more work on its stack. Also the recursion in the BDD operations is eliminated by placing the work of a recursive operation on the stack. This looks a bit like the fine-grained parallelism implemented in \cite{dijk2012parallelization}. The algorithm terminates when all stacks become empty.

The researchers used the \emph{ISCAS} benchmark \cite{brglez1985neutral} on a cluster of 32 nodes to test their implementations. Compared to the sequential version, speedups of 20 to 32 were obtained. In some cases however, the BDD did not fit into the memory of the workstation running the sequential implementation, which caused good speedups.
